---layout: '../../layouts/MarkdownPost.astro'title: 'Label Noise: Correcting a Correction'pubDate: 2023-07-24description: 'Automated track arxiv-daily latest papers around arxiv paper daily template'author: 'William Toner et.al.'cover:    url: 'https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg'    square: 'https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg'    alt: 'cover'tags: ['brand','brand monitor']theme: 'light'featured: true
meta: - name: author   content: 作者是我 - name: keywords   content: key3, key4keywords: key1, key2, key3---## authors:William Toner et.al. Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
