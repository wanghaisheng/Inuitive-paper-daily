---layout: '../../layouts/MarkdownPost.astro'title: 'XDLM: Cross-lingual Diffusion Language Model for Machine Translation'pubDate: 2023-07-25description: 'Automated track arxiv-daily latest papers around arxiv paper daily template'author: 'Linyao Chen et.al.'cover:    url: 'https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg'    square: 'https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg'    alt: 'cover'tags: ['brand','brand monitor']theme: 'light'featured: true<br/><br/>meta: - name: author   content: 作者是我 - name: keywords   content: key3, key4keywords: key1, key2, key3---## authors:Linyao Chen et.al. Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages. In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.
